<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Ceph</title>
    <url>/2021/10/05/ceph/</url>
    <content><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Ceph is an open source storage system, which supports 3 types of sorage:</p>
<ul>
<li>block storage: support snapshot</li>
<li>file system: posix interface, support snapshot</li>
<li>object storage: s3 compatible</li>
</ul>
<span id="more"></span>
<h3 id="Core-components-and-concepts"><a href="#Core-components-and-concepts" class="headerlink" title="Core components and concepts"></a>Core components and concepts</h3><img src="/2021/10/05/ceph/ceph-architecture.png" class="" title="Ceph Architecture">

<ul>
<li><strong>Monitor</strong>: maintains the status of the cluster (monitor map, manager map, OSD map, CRUSH map)</li>
<li><strong>OSD</strong>: Object Storage Device, a daemon which interacts with client for providing data</li>
<li><strong>MDS</strong>: Ceph Meta  Data Server, meta data service for CephFS</li>
<li><strong>RGW</strong>: Rados Gateway, provide object storage service</li>
<li><strong>RBD</strong>: Rados Block Device, block storage service</li>
<li><strong>CRUSH</strong>:  algorithm for data distribution in Ceph</li>
<li><strong>PG</strong>: Placement Groups, a logical concept for better data distribution and localization</li>
</ul>
<h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><h4 id="Cephadm"><a href="#Cephadm" class="headerlink" title="Cephadm"></a><a href="https://help.aliyun.com/document_detail/147650.html#title-s05-xk0-670">Cephadm</a></h4><h3 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h3><h4 id="CephRBD-use-ceph-in-k8s"><a href="#CephRBD-use-ceph-in-k8s" class="headerlink" title="CephRBD (use ceph in k8s)"></a>CephRBD (use ceph in k8s)</h4><ul>
<li><p>prepare ceph server</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create a pool for block storage</span></span><br><span class="line">$ ceph osd pool create kubernetes</span><br><span class="line"><span class="comment"># create user</span></span><br><span class="line">$ ceph auth get-or-create client.kubernetes mon <span class="string">&#x27;profile rbd&#x27;</span> osd <span class="string">&#x27;profile rbd pool=kubernetes&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li><p>prepare rbd dynamic provisioner and plugin</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># refer to https://github.com/ceph/ceph-csi/tree/devel/deploy/rbd/kubernetes</span></span><br><span class="line">$ kubectl apply -f ceph-block-provisioner/deploy/ -n ceph</span><br></pre></td></tr></table></figure></li>
<li><p>test dynamic provisioning of rbd of block mode</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f test-raw-block-pvc.yaml</span><br><span class="line">$ kubectl apply -f test-raw-block-pod.yaml</span><br></pre></td></tr></table></figure></li>
<li><p>test dynamic provisioning of rbd of filesystem mode</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f test-filesystem-pvc.yaml</span><br><span class="line">$ kubectl apply -f test-filesystem-pod.yaml</span><br></pre></td></tr></table></figure></li>
<li><p><strong>Note</strong>: </p>
<ul>
<li>ERROR: MountVolume.SetUp failed for volume “registration-dir” : hostPath type check failed: /var/lib/kubelet/plugins_registry/ is not a directory.<br><strong>change kubelet dir to <code>/data/kubelet</code></strong></li>
<li>rbd-plugin is a daemonset, add tolerations to deploy to all nodes  <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;master&quot;</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;prometheus&quot;</span></span><br><span class="line">    <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;master&quot;</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;nowork&quot;</span></span><br><span class="line">    <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h4 id="CephFS-use-cephfs-in-k8s"><a href="#CephFS-use-cephfs-in-k8s" class="headerlink" title="CephFS (use cephfs in k8s)"></a>CephFS (use cephfs in k8s)</h4><ul>
<li><p>prepare cephfs server</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create a fs, refer to cephadm</span></span><br><span class="line"><span class="comment"># create pool for data and metadata </span></span><br><span class="line">$ ceph osd pool create cephfs_data 64 64</span><br><span class="line">$ ceph osd pool create cephfs_metadata 64 64</span><br><span class="line"><span class="comment"># create a fs</span></span><br><span class="line">$ ceph fs new cephfs-demo1 cephfs_metadata cephfs_data</span><br><span class="line">$ ceph orch apply mds cephfs-demo1 --placement=<span class="string">&quot;3 iam02 openam01 opendj01&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p>prepare cephfs dynamic provisioner and plugin </p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># refer to https://github.com/ceph/ceph-csi/tree/devel/deploy/cephfs/kubernetes</span></span><br><span class="line">$ kubectl apply -f ceph-block-provisioner/deploy/ -n ceph</span><br><span class="line"></span><br><span class="line"><span class="comment"># test cephfs pvc</span></span><br><span class="line">kubectl apply -f test_pvc.yaml</span><br><span class="line">kubectl apply -f test_pvc_pod.yaml</span><br><span class="line"></span><br><span class="line">ceph -m 10.0.2.4:6789 --id admin --key=AQCEKzthORrmJhAA3PH7m+9kldDLLRQXuscofg== -c /etc/ceph/ceph.conf fs ls --format=json</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="Ceph-Object-Storage"><a href="#Ceph-Object-Storage" class="headerlink" title="Ceph Object Storage"></a>Ceph Object Storage</h4><h3 id="Ceph-Tutorials"><a href="#Ceph-Tutorials" class="headerlink" title="Ceph Tutorials"></a>Ceph Tutorials</h3><ul>
<li>cheat sheet of ceph command<br>  <a href="https://sabaini.at/pages/ceph-cheatsheet.html">https://sabaini.at/pages/ceph-cheatsheet.html</a></li>
</ul>
<p><strong>Note</strong>:<br>Ceph RBD and CephFS both supports filesystem</p>
<ul>
<li>ceph rbd: supports the volume shared by pods running on the same node, low latency, good I/O</li>
<li>cephfs: supports the volume shared by pods running across multiple nodes, higher latency, good I/O, expecially for large files, but bottleneck with more storage</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://www.cnblogs.com/hukey/p/11899710.html">https://www.cnblogs.com/hukey/p/11899710.html</a></li>
<li><a href="https://dylanyang.top/post/2021/05/15/k8s%E4%BD%BF%E7%94%A8ceph-csi%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8cephfs/">https://dylanyang.top/post/2021/05/15/k8s%E4%BD%BF%E7%94%A8ceph-csi%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8cephfs/</a></li>
<li><a href="https://blog.51cto.com/leejia/2583381">https://blog.51cto.com/leejia/2583381</a></li>
<li><a href="https://docs.ceph.com/en/octopus/rbd/rbd-kubernetes/">https://docs.ceph.com/en/octopus/rbd/rbd-kubernetes/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">https://kubernetes.io/docs/concepts/storage/persistent-volumes/</a></li>
</ol>
]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>cloud</tag>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title>Install ceph with cephadm</title>
    <url>/2021/10/05/cephadm/</url>
    <content><![CDATA[<h2 id="Layout"><a href="#Layout" class="headerlink" title="Layout"></a>Layout</h2><p>10.0.2.7 iam02 /dev/sda mon<br>10.0.2.4 openam01 /dev/sdc mon<br>10.0.2.5 opendj01 /dev/sdb mon</p>
<span id="more"></span>
<h2 id="Prepare-Environment"><a href="#Prepare-Environment" class="headerlink" title="Prepare Environment"></a>Prepare Environment</h2><ul>
<li>Ntp</li>
<li>Python3 </li>
<li>Container Runtime</li>
<li>ssh environment</li>
<li>Repo Source  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ wget --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm</span><br><span class="line">$ chmod +x cephadm</span><br><span class="line">$ ./cephadm add-repo --release pacific</span><br><span class="line">$ ./cephadm install</span><br><span class="line">$ apt-get update</span><br></pre></td></tr></table></figure>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2>Note: Try to install with root user to avoid some issues<h3 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephadm bootstrap --mon-ip 10.0.2.7  --cluster-network 10.0.2.0/24</span><br><span class="line"><span class="comment"># Note: store the dashboard login credentials in the initialization logs</span></span><br><span class="line"><span class="comment"># create user for dashboard</span></span><br><span class="line">$ ceph dashboard set-login-credentials yuanjing -i secret-yuanjing</span><br></pre></td></tr></table></figure>
<h3 id="Access-cluster"><a href="#Access-cluster" class="headerlink" title="Access cluster"></a>Access cluster</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check cluster status</span></span><br><span class="line">$ cephadm shell -- ceph -s</span><br><span class="line"><span class="comment"># check osd status</span></span><br><span class="line">$ cephadm shell -- ceph osd tree</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Add-other-nodes-to-cluster"><a href="#Add-other-nodes-to-cluster" class="headerlink" title="Add other nodes to cluster"></a>Add other nodes to cluster</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># copy ssh pub key (ceph.pub) generated by ceph to other nodes</span></span><br><span class="line">$ ssh-copy-id -f -i /etc/ceph/ceph.pub openam01</span><br><span class="line">$ ssh-copy-id -f -i /etc/ceph/ceph.pub opendj01</span><br><span class="line"><span class="comment"># add nodes</span></span><br><span class="line">$ ceph orch host add openam01 10.0.2.4</span><br><span class="line">$ ceph orch host add opendj01 10.0.2.5</span><br><span class="line"></span><br><span class="line"><span class="comment"># check cluster status (mon and mgr will be deployed on nodes automatically)</span></span><br><span class="line">$ ceph -s</span><br></pre></td></tr></table></figure>
<h3 id="Deploy-osd"><a href="#Deploy-osd" class="headerlink" title="Deploy osd"></a>Deploy osd</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check available devices</span></span><br><span class="line">$ os orch device ls</span><br><span class="line"><span class="comment"># A storage device is considered available if all of the following conditions are met:</span></span><br><span class="line"><span class="comment"># The device must have no partitions.</span></span><br><span class="line"><span class="comment"># The device must not have any LVM state.</span></span><br><span class="line"><span class="comment"># The device must not be mounted.</span></span><br><span class="line"><span class="comment"># The device must not contain a file system.</span></span><br><span class="line"><span class="comment"># The device must not contain a Ceph BlueStore OSD.</span></span><br><span class="line"><span class="comment"># The device must be larger than 5 GB.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># deploy all available devices</span></span><br><span class="line">$ ceph orch apply osd --all-available-devices</span><br><span class="line"></span><br><span class="line"><span class="comment"># add new osd</span></span><br><span class="line">$ ceph orch daemon add osd hostname:/dev/sdb</span><br></pre></td></tr></table></figure>
<h3 id="Deploy-mds"><a href="#Deploy-mds" class="headerlink" title="Deploy mds"></a>Deploy mds</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create pool for data and metadata </span></span><br><span class="line">$ ceph osd pool create cephfs_data 64 64</span><br><span class="line">$ ceph osd pool create cephfs_metadata 64 64</span><br><span class="line"><span class="comment"># create a fs</span></span><br><span class="line">$ ceph fs new cephfs-demo1 cephfs_metadata cephfs_data</span><br><span class="line"><span class="comment"># fs service</span></span><br><span class="line">$ ceph orch apply mds cephfs-demo1 --placement=<span class="string">&quot;3 iam02 openam01 opendj01&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or create with one command</span></span><br><span class="line">$ ceph fs volume create cephfs-demo2 <span class="string">&quot;iam02,openam01,opendj01&quot;</span></span><br><span class="line"><span class="comment"># pool will be automatically created in this case with default pg and pgp, check</span></span><br><span class="line">$ ceph fs ls</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Ceph-CLI"><a href="#Ceph-CLI" class="headerlink" title="Ceph CLI"></a>Ceph CLI</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># install ceph-common on client nodes</span></span><br><span class="line">$ cephadm add repo --release &#123;pacific&#125;</span><br><span class="line">$ cephadm install ceph-common</span><br><span class="line"><span class="comment"># on client nodes, we could use ceph command</span></span><br><span class="line">$ ceph -s</span><br><span class="line">$ ceph orch host ls</span><br></pre></td></tr></table></figure>

<h2 id="Ceph-as-persistent-storage-for-k8s"><a href="#Ceph-as-persistent-storage-for-k8s" class="headerlink" title="Ceph as persistent storage for k8s"></a>Ceph as persistent storage for k8s</h2><h3 id="Use-CephFS"><a href="#Use-CephFS" class="headerlink" title="Use CephFS"></a>Use CephFS</h3><h3 id="Use-Ceph-RBD"><a href="#Use-Ceph-RBD" class="headerlink" title="Use Ceph RBD"></a>Use Ceph RBD</h3><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://www.cnblogs.com/zjz20/p/14136349.html">https://www.cnblogs.com/zjz20/p/14136349.html</a></li>
</ol>
]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>cloud</tag>
        <tag>ceph</tag>
        <tag>cephadm</tag>
      </tags>
  </entry>
  <entry>
    <title>Install ceph with rook in k8s</title>
    <url>/2021/10/06/rook-ceph/</url>
    <content><![CDATA[<h2 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h2><p>For installing ceph in k8s with rook, follwing requirements should be fulfilled</p>
<ul>
<li>kubernetes &gt; v1.17.0</li>
<li>minimum 3 nodes of kubernetes cluster</li>
<li>disk  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1. The device must have no partitions.</span><br><span class="line">2. The device must not have any LVM state.</span><br><span class="line">3. The device must not be mounted.</span><br><span class="line">4. The device must not contain a file system.</span><br><span class="line">5. The device must not contain a Ceph BlueStore OSD.</span><br><span class="line">6. The device must be larger than 5 GB.</span><br><span class="line"><span class="comment"># check with lsblk</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Install-Rook-Operator"><a href="#Install-Rook-Operator" class="headerlink" title="Install Rook Operator"></a>Install Rook Operator</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># download rook release 1.3</span></span><br><span class="line">$ git <span class="built_in">clone</span> --single-branch --branch release-1.3 https://github.com/rook/rook.git</span><br><span class="line"><span class="comment"># configuration files</span></span><br><span class="line">$ <span class="built_in">cd</span> rook/cluster/examples/kubernetes/ceph</span><br><span class="line"></span><br><span class="line">$ kubectl create -f common.yaml</span><br><span class="line">$ kubectl apply -f operator.yaml</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">$ configmap/rook-ceph-operator-config created</span><br><span class="line">$ deployment.apps/rook-ceph-operator created</span><br><span class="line"></span><br><span class="line"><span class="comment">#check pods created in rook-ceph</span></span><br><span class="line">$ kubectl get pod -n rook-ceph</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">rook-ceph-operator-599765ff49-fhbz9   1/1     Running   0          92s</span><br><span class="line">rook-discover-6fhlb                   1/1     Running   0          55s</span><br><span class="line">rook-discover-97kmz                   1/1     Running   0          55s</span><br><span class="line">rook-discover-z5k2z                   1/1     Running   0          55s</span><br></pre></td></tr></table></figure>

<h2 id="Create-a-ceph-cluster"><a href="#Create-a-ceph-cluster" class="headerlink" title="Create a ceph cluster"></a>Create a ceph cluster</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># label your node for ceph setup</span></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl taint node k8smaster01 node-role.kubernetes.io/master=&quot;&quot;:NoSchedule</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl taint node k8smaster02 node-role.kubernetes.io/master=&quot;&quot;:NoSchedule</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl taint node k8smaster03 node-role.kubernetes.io/master=&quot;&quot;:NoSchedule</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl label nodes &#123;192.168.10.127,192.168.10.128,192.168.10.129&#125; ceph-osd=enabled</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl label nodes &#123;192.168.10.127,192.168.10.128,192.168.10.129&#125; ceph-mon=enabled</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl label nodes 192.168.10.127 ceph-mgr=enabled</span></span><br><span class="line"></span><br><span class="line">**提示：当前版本rook中mgr只能支持一个节点运行。**</span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ~]<span class="comment"># cd /root/rook/cluster/examples/kubernetes/ceph/</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl create -f common.yaml</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl create -f operator.yaml</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kubectl apply -f cephcluster.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">NAME                                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">csi-cephfsplugin-lz6dn                                 3/3     Running   0          3m54s</span><br><span class="line">csi-cephfsplugin-provisioner-674847b584-4j9jw          5/5     Running   0          3m54s</span><br><span class="line">csi-cephfsplugin-provisioner-674847b584-h2cgl          5/5     Running   0          3m54s</span><br><span class="line">csi-cephfsplugin-qbpnq                                 3/3     Running   0          3m54s</span><br><span class="line">csi-cephfsplugin-qzsvr                                 3/3     Running   0          3m54s</span><br><span class="line">csi-rbdplugin-kk9sw                                    3/3     Running   0          3m55s</span><br><span class="line">csi-rbdplugin-l95f8                                    3/3     Running   0          3m55s</span><br><span class="line">csi-rbdplugin-provisioner-64ccb796cf-8gjwv             6/6     Running   0          3m55s</span><br><span class="line">csi-rbdplugin-provisioner-64ccb796cf-dhpwt             6/6     Running   0          3m55s</span><br><span class="line">csi-rbdplugin-v4hk6                                    3/3     Running   0          3m55s</span><br><span class="line">rook-ceph-crashcollector-pool-33zy7-68cdfb6bcf-9cfkn   1/1     Running   0          109s</span><br><span class="line">rook-ceph-crashcollector-pool-33zyc-565559f7-7r6rt     1/1     Running   0          53s</span><br><span class="line">rook-ceph-crashcollector-pool-33zym-749dcdc9df-w4xzl   1/1     Running   0          78s</span><br><span class="line">rook-ceph-mgr-a-7fdf77cf8d-ppkwl                       1/1     Running   0          53s</span><br><span class="line">rook-ceph-mon-a-97d9767c6-5ftfm                        1/1     Running   0          109s</span><br><span class="line">rook-ceph-mon-b-9cb7bdb54-lhfkj                        1/1     Running   0          96s</span><br><span class="line">rook-ceph-mon-c-786b9f7f4b-jdls4                       1/1     Running   0          78s</span><br><span class="line">rook-ceph-operator-599765ff49-fhbz9                    1/1     Running   0          6m58s</span><br><span class="line">rook-ceph-osd-prepare-pool-33zy7-c2hww                 1/1     Running   0          21s</span><br><span class="line">rook-ceph-osd-prepare-pool-33zyc-szwsc                 1/1     Running   0          21s</span><br><span class="line">rook-ceph-osd-prepare-pool-33zym-2p68b                 1/1     Running   0          21s</span><br><span class="line">rook-discover-6fhlb                                    1/1     Running   0          6m21s</span><br><span class="line">rook-discover-97kmz                                    1/1     Running   0          6m21s</span><br><span class="line">rook-discover-z5k2z                                    1/1     Running   0          6m21s</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Create-a-block-storageclass"><a href="#Create-a-block-storageclass" class="headerlink" title="Create a block storageclass"></a>Create a block storageclass</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f ./csi/rbd/storageclass.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">cephblockpool.ceph.rook.io/replicapool created</span><br><span class="line">storageclass.storage.k8s.io/rook-ceph-block created</span><br></pre></td></tr></table></figure>


<h4 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h4><ol>
<li><p><a href="https://kuboard.cn/learning/k8s-intermediate/persistent/ceph/rook-config.html#%E5%AE%89%E8%A3%85-rook-ceph">https://kuboard.cn/learning/k8s-intermediate/persistent/ceph/rook-config.html#%E5%AE%89%E8%A3%85-rook-ceph</a></p>
</li>
<li><p><a href="https://www.joyk.com/dig/detail/1585287051590150">https://www.joyk.com/dig/detail/1585287051590150</a></p>
</li>
<li><p><a href="https://www.qikqiak.com/post/deploy-ceph-cluster-with-rook/">https://www.qikqiak.com/post/deploy-ceph-cluster-with-rook/</a></p>
</li>
<li><p><a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-ceph-cluster-within-kubernetes-using-rook">https://www.digitalocean.com/community/tutorials/how-to-set-up-a-ceph-cluster-within-kubernetes-using-rook</a></p>
</li>
<li><p><a href="https://xiebiao.top/post/storage/rook_install_operator/">https://xiebiao.top/post/storage/rook_install_operator/</a></p>
</li>
<li><p><a href="https://github.com/rook/rook/tree/master/cluster/examples">https://github.com/rook/rook/tree/master/cluster/examples</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>cloud</tag>
        <tag>ceph</tag>
        <tag>rook-ceph</tag>
      </tags>
  </entry>
</search>
