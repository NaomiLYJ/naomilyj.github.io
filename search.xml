<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Ceph</title>
    <url>/2021/10/05/ceph/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Ceph is an open source storage system, which supports 3 types of sorage:</p>
<ul>
<li>block storage: support snapshot</li>
<li>file system: posix interface, support snapshot</li>
<li>object storage: s3 compatible</li>
</ul>
<span id="more"></span>
<h2 id="Core-components-and-concepts"><a href="#Core-components-and-concepts" class="headerlink" title="Core components and concepts"></a>Core components and concepts</h2><img src="/2021/10/05/ceph/ceph-architecture.png" class="" title="Ceph Architecture">

<ul>
<li><strong>Monitor</strong>: maintains the status of the cluster (monitor map, manager map, OSD map, CRUSH map)</li>
<li><strong>OSD</strong>: Object Storage Device, a daemon which interacts with client for providing data</li>
<li><strong>MDS</strong>: Ceph Meta  Data Server, meta data service for CephFS</li>
<li><strong>RGW</strong>: Rados Gateway, provide object storage service</li>
<li><strong>RBD</strong>: Rados Block Device, block storage service</li>
<li><strong>CRUSH</strong>:  algorithm for data distribution in Ceph</li>
<li><strong>PG</strong>: Placement Groups, a logical concept for better data distribution and localization</li>
</ul>
<h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><h3 id="1-Cephadm"><a href="#1-Cephadm" class="headerlink" title="1. Cephadm"></a>1. Cephadm</h3><p>For setup with cephadm please go to <a href="https://naomilyj.github.io/2021/10/05/cephadm/">here</a></p>
<h3 id="2-Rook-ceph"><a href="#2-Rook-ceph" class="headerlink" title="2. Rook-ceph"></a>2. Rook-ceph</h3><p>For setup with rook please go to <a href="https://naomilyj.github.io/2021/10/06/rook-ceph/">here</a></p>
<h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><h3 id="1-CephRBD-use-ceph-in-k8s"><a href="#1-CephRBD-use-ceph-in-k8s" class="headerlink" title="1. CephRBD (use ceph in k8s)"></a>1. CephRBD (use ceph in k8s)</h3><ul>
<li><p>prepare ceph server</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create a pool for block storage</span></span><br><span class="line">$ ceph osd pool create kubernetes</span><br><span class="line"><span class="comment"># create user</span></span><br><span class="line">$ ceph auth get-or-create client.kubernetes mon <span class="string">&#x27;profile rbd&#x27;</span> osd <span class="string">&#x27;profile rbd pool=kubernetes&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li><p>prepare rbd dynamic provisioner and plugin</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># refer to https://github.com/ceph/ceph-csi/tree/devel/deploy/rbd/kubernetes</span></span><br><span class="line">$ kubectl apply -f ceph-block-provisioner/deploy/ -n ceph</span><br></pre></td></tr></table></figure></li>
<li><p>test dynamic provisioning of rbd of block mode</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f test-raw-block-pvc.yaml</span><br><span class="line">$ kubectl apply -f test-raw-block-pod.yaml</span><br></pre></td></tr></table></figure></li>
<li><p>test dynamic provisioning of rbd of filesystem mode</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f test-filesystem-pvc.yaml</span><br><span class="line">$ kubectl apply -f test-filesystem-pod.yaml</span><br></pre></td></tr></table></figure></li>
<li><p><strong>Note</strong>: </p>
<ul>
<li>ERROR: MountVolume.SetUp failed for volume “registration-dir” : hostPath type check failed: /var/lib/kubelet/plugins_registry/ is not a directory.<br><strong>change kubelet dir to <code>/data/kubelet</code></strong></li>
<li>rbd-plugin is a daemonset, add tolerations to deploy to all nodes  <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;master&quot;</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;prometheus&quot;</span></span><br><span class="line">    <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;master&quot;</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;nowork&quot;</span></span><br><span class="line">    <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="2-CephFS-use-cephfs-in-k8s"><a href="#2-CephFS-use-cephfs-in-k8s" class="headerlink" title="2. CephFS (use cephfs in k8s)"></a>2. CephFS (use cephfs in k8s)</h3><ul>
<li><p>cephfs structure</p>
<img src="/2021/10/05/ceph/cephfs.png" class="" title="CephFS"></li>
<li><p>prepare cephfs server</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create pool for data and metadata </span></span><br><span class="line">$ ceph osd pool create cephfs_data 64 64</span><br><span class="line">$ ceph osd pool create cephfs_metadata 64 64</span><br><span class="line"><span class="comment"># create a fs</span></span><br><span class="line">$ ceph fs new cephfs-demo1 cephfs_metadata cephfs_data</span><br><span class="line">$ ceph orch apply mds cephfs-demo1 --placement=<span class="string">&quot;3 iam02 openam01 opendj01&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p>prepare cephfs dynamic provisioner and plugin (skip it when installing with rook-ceph)</p>
  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># refer to https://github.com/ceph/ceph-csi/tree/devel/deploy/cephfs/kubernetes</span></span><br><span class="line">$ kubectl apply -f ceph-block-provisioner/deploy/ -n ceph</span><br><span class="line"></span><br><span class="line"><span class="comment"># test cephfs pvc</span></span><br><span class="line">kubectl apply -f test_pvc.yaml</span><br><span class="line">kubectl apply -f test_pvc_pod.yaml</span><br><span class="line"></span><br><span class="line">ceph -m 10.0.2.4:6789 --id admin --key=AQCEKzthORrmJhAA3PH7m+9kldDLLRQXuscofg== -c /etc/ceph/ceph.conf fs ls --format=json</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-Ceph-Object-Storage"><a href="#3-Ceph-Object-Storage" class="headerlink" title="3. Ceph Object Storage"></a>3. Ceph Object Storage</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># install s3cmd</span></span><br><span class="line">$ apt-get install s3cmd</span><br><span class="line"><span class="comment"># configure with key and secret, dns-style:  rgw-ip:port/%(bucket)s</span></span><br><span class="line">$ s3cmd --configure</span><br><span class="line"><span class="comment"># create bucket</span></span><br><span class="line">$ s3cmd mb s3://demo-bucket</span><br></pre></td></tr></table></figure>

<h2 id="Ceph-Tutorials"><a href="#Ceph-Tutorials" class="headerlink" title="Ceph Tutorials"></a>Ceph Tutorials</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  pool</span></span><br><span class="line">$ ceph osd pool ls detail</span><br><span class="line">$ ceph config <span class="built_in">set</span> mon mon_allow_pool_delete <span class="literal">true</span></span><br><span class="line">$ ceph osd pool rm testpg testpg --yes-i-really-really-mean-it</span><br></pre></td></tr></table></figure>
<ul>
<li>cheat sheet of ceph command<br>  <a href="https://sabaini.at/pages/ceph-cheatsheet.html">https://sabaini.at/pages/ceph-cheatsheet.html</a></li>
</ul>
<h2 id="Ceph-crushmap"><a href="#Ceph-crushmap" class="headerlink" title="Ceph crushmap"></a>Ceph crushmap</h2><p><strong>Note</strong>:<br>Ceph RBD and CephFS both supports filesystem</p>
<ul>
<li>ceph rbd: supports the volume shared by pods running on the same node, low latency, good I/O</li>
<li>cephfs: supports the volume shared by pods running across multiple nodes, higher latency, good I/O, expecially for large files, but bottleneck with more storage</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://www.cnblogs.com/hukey/p/11899710.html">https://www.cnblogs.com/hukey/p/11899710.html</a></li>
<li><a href="https://dylanyang.top/post/2021/05/15/k8s%E4%BD%BF%E7%94%A8ceph-csi%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8cephfs/">https://dylanyang.top/post/2021/05/15/k8s%E4%BD%BF%E7%94%A8ceph-csi%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8cephfs/</a></li>
<li><a href="https://blog.51cto.com/leejia/2583381">https://blog.51cto.com/leejia/2583381</a></li>
<li><a href="https://docs.ceph.com/en/octopus/rbd/rbd-kubernetes/">https://docs.ceph.com/en/octopus/rbd/rbd-kubernetes/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">https://kubernetes.io/docs/concepts/storage/persistent-volumes/</a></li>
<li><a href="http://www.xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/">http://www.xuxiaopang.com/2016/11/08/easy-ceph-CRUSH/</a></li>
</ol>
]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>storage</tag>
        <tag>cloud</tag>
        <tag>ceph</tag>
      </tags>
  </entry>
  <entry>
    <title>helm charts</title>
    <url>/2022/02/06/helmchart/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This article is about</p>
<ul>
<li>what is helm charts</li>
<li>how to use helm charts</li>
<li>how to create helm charts</li>
<li>helm charts with helm secrets</li>
</ul>
<p>based on helm <code>v3.7.1</code></p>
<p>You can install helm with </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl -sSL https://get.helm.sh/helm-v3.7.1-linux-amd64.tar.gz | tar --strip-components 1 -xzC /usr/bin linux-amd64/helm &amp;&amp; chmod +x /usr/bin/helm</span><br></pre></td></tr></table></figure>
<span id="more"></span>


<h2 id="What-is-helm-charts"><a href="#What-is-helm-charts" class="headerlink" title="What is helm charts"></a>What is helm charts</h2><p>According to definition from helm official website (<a href="https://helm.sh/">https://helm.sh</a>), <code>helm</code> helps you manage Kubernetes applications, and <code>helm charts</code> help you define, install, and upgrade even the most complex Kubernetes application.</p>
<p>Helm charts package all the k8s manifests (YAML files) of your application and make your management of metadata more easily. A helm chart has the following structure</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mychart</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── charts <span class="comment"># contains other sub charts</span></span><br><span class="line">├── templates <span class="comment"># chart templates (gotemplates)，which are used to render the final k8s manifests with variables defined in values.yaml</span></span><br><span class="line">│   ├── NOTES.txt <span class="comment"># hint message when the user run command helm install</span></span><br><span class="line">│   ├── _helpers.tpl <span class="comment"># some helper code for templates</span></span><br><span class="line">│   ├── xx.yaml <span class="comment"># Kubernetes manifest</span></span><br><span class="line">│   ├── ... <span class="comment"># Kubernetes manifest</span></span><br><span class="line">│   └── tests</span><br><span class="line">│       └── test-connection.yaml</span><br><span class="line">└── values.yaml <span class="comment"># variables for rendering templates when the user run command helm install</span></span><br></pre></td></tr></table></figure>

<h2 id="How-to-use-helm-charts"><a href="#How-to-use-helm-charts" class="headerlink" title="How to use helm charts"></a>How to use helm charts</h2><h3 id="1-local-chart"><a href="#1-local-chart" class="headerlink" title="1. local chart"></a>1. local chart</h3><p>You could install your local helm chart with </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ helm install your-release-name /path/to/your/chart --namespace your-namespace -f /path/to/your/values.yaml</span><br><span class="line"><span class="comment"># Upgrade</span></span><br><span class="line">$ helm upgrade --install your-release-name /path/to/your/chart --namespace your-namespace -f /path/to/your/values.yaml</span><br></pre></td></tr></table></figure>

<h3 id="2-chart-from-repository"><a href="#2-chart-from-repository" class="headerlink" title="2. chart from repository"></a>2. chart from repository</h3><p>To install public helm chart, you have to add public helm chart repository</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Add a public repo</span></span><br><span class="line">$ helm repo add repo-name repo-url</span><br><span class="line"><span class="comment"># Add a private repo with self-signed certificate</span></span><br><span class="line">$ helm repo add repo-name repo-url --insecure-skip-tls-verify</span><br><span class="line"><span class="comment"># e.g. add a harbor chart repository</span></span><br><span class="line">$ helm repo add myharbor https://harbor.mydomain.com/chartrepo/library --insecure-skip-tls-verify</span><br><span class="line"><span class="comment"># Login to your private registry</span></span><br><span class="line">$ helm registry login https://harbor.mydomain.com -u xxx -p xxx --insecure</span><br><span class="line"></span><br><span class="line"><span class="comment"># List all helm repositories in your local cache</span></span><br><span class="line">$ helm repo list</span><br><span class="line"><span class="comment"># Update your local Helm chart repository cache</span></span><br><span class="line">$ helm repo update</span><br><span class="line"></span><br><span class="line"><span class="comment"># Search all charts under a repo</span></span><br><span class="line">$ helm search repo repo-name</span><br><span class="line"><span class="comment"># Download a chart package from helm repository</span></span><br><span class="line">$ helm fetch repo-name/chart-name --verion=xxx --insecure-skip-tls-verify</span><br><span class="line"></span><br><span class="line"><span class="comment"># Install a helm chart</span></span><br><span class="line">$ helm install --cleanup-on-fail your-release-name repo-name/chart-name --namespace your-namespace --version xxx -f /path/to/your/values.yaml</span><br><span class="line"><span class="comment"># Upgrade </span></span><br><span class="line">$ helm upgrade --install --cleanup-on-fail your-release-name repo-name/chart-name --namespace your-namespace --version xxx -f /path/to/your/values.yaml</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="How-to-create-helm-charts"><a href="#How-to-create-helm-charts" class="headerlink" title="How to create helm charts"></a>How to create helm charts</h2><h3 id="Create"><a href="#Create" class="headerlink" title="Create"></a>Create</h3><p>To create a helm chart, you could simply run </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ helm create foo</span><br><span class="line"><span class="comment"># The following files will be generated</span></span><br><span class="line">foo/</span><br><span class="line">├── .helmignore   <span class="comment"># Contains patterns to ignore when packaging Helm charts.</span></span><br><span class="line">├── Chart.yaml    <span class="comment"># Information about your chart</span></span><br><span class="line">├── values.yaml   <span class="comment"># The default values for your templates</span></span><br><span class="line">├── charts/       <span class="comment"># Charts that this chart depends on</span></span><br><span class="line">└── templates/    <span class="comment"># The template files</span></span><br><span class="line">    └── tests/    <span class="comment"># The test files</span></span><br></pre></td></tr></table></figure>

<p>For details of helm chart grammar, please refer to []</p>
<h3 id="Lint"><a href="#Lint" class="headerlink" title="Lint"></a>Lint</h3><p>To lint your chart</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ helm lint /path/to/your/chart</span><br></pre></td></tr></table></figure>

<h3 id="Package-and-Push"><a href="#Package-and-Push" class="headerlink" title="Package and Push"></a>Package and Push</h3><p>To package your chart and push to helm chart registry, there are 2 methods</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Method 1</span></span><br><span class="line">$ helm package /path/to/your/chart</span><br><span class="line">Successfully packaged chart and saved it to: /my/path/hello-world-0.1.0.tgz</span><br><span class="line">$ helm push hello-world-0.1.0.tgz oci://harbor.mydomain.com/chartrepo/library --insecure-skip-tls-verify</span><br><span class="line"><span class="comment"># Directly push directory</span></span><br><span class="line">$ helm push /path/to/your/chart oci://harbor.mydomain.com/chartrepo/library --insecure-skip-tls-verify</span><br><span class="line"></span><br><span class="line"><span class="comment"># Method 2 (old version and is removed)</span></span><br><span class="line">$ helm chart save /path/to/your/chart harbor.mydomain.com/chartrepo/library/chart-name:chart-version</span><br><span class="line">$ helm chart push harbor.mydomain.com/chartrepo/library/chart-name:chart-version</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note, before push, you need to login to helm registry and</span></span><br><span class="line">$ <span class="built_in">export</span> HELM_EXPERIMENTAL_OCI=1</span><br><span class="line">$ helm plugin install https://github.com/chartmuseum/helm-push.git --version master</span><br></pre></td></tr></table></figure>

<h2 id="helm-charts-with-helm-secrets"><a href="#helm-charts-with-helm-secrets" class="headerlink" title="helm charts with helm secrets"></a>helm charts with helm secrets</h2><p>Some variables in <code>values.yaml</code> are sensitive data, e.g, passwords, which should be encrypted. <a href="https://github.com/jkroepke/helm-secrets/wiki/Usage">helm secrets</a> is a helm plugin to do this job.</p>
<p>This plugin provides ability to encrypt/decrypt secrets files to store in less secure places, before they are installed using<br>Helm. To decrypt/encrypt/edit you need to initialize/first encrypt secrets with sops - <a href="https://github.com/mozilla/sops">https://github.com/mozilla/sops</a></p>
<h3 id="To-install-sops-and-helm-secrets"><a href="#To-install-sops-and-helm-secrets" class="headerlink" title="To install sops and helm secrets"></a>To install <code>sops</code> and <code>helm secrets</code></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl -O -L -C - https://github.com/mozilla/sops/releases/download/v3.7.1/sops-v3.7.1.linux -o /usr/bin/sops &amp;&amp; chmod +x /usr/bin/sops</span><br><span class="line">$ helm plugin install https://github.com/jkroepke/helm-secrets --version v3.11.0</span><br></pre></td></tr></table></figure>

<h3 id="To-generate-keys-for-encryption"><a href="#To-generate-keys-for-encryption" class="headerlink" title="To generate keys for encryption"></a>To generate keys for encryption</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ gpg --gen-key</span><br><span class="line">enter username, email and password</span><br><span class="line"></span><br><span class="line">gpg (GnuPG) 2.2.27; Copyright (C) 2021 Free Software Foundation, Inc.</span><br><span class="line">This is free software: you are free to change and redistribute it.</span><br><span class="line">There is NO WARRANTY, to the extent permitted by law.</span><br><span class="line"></span><br><span class="line">Note: Use <span class="string">&quot;gpg --full-generate-key&quot;</span> <span class="keyword">for</span> a full featured key generation dialog.</span><br><span class="line"></span><br><span class="line">GnuPG needs to construct a user ID to identify your key.</span><br><span class="line"></span><br><span class="line">Real name: user-name</span><br><span class="line">Email address: user-name@example.com</span><br><span class="line">You selected this USER-ID:</span><br><span class="line">    <span class="string">&quot;user-name &lt;user-name@example.com&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">Change (N)ame, (E)mail, or (O)kay/(Q)uit? O</span><br><span class="line">We need to generate a lot of random bytes. It is a good idea to perform</span><br><span class="line">some other action (<span class="built_in">type</span> on the keyboard, move the mouse, utilize the</span><br><span class="line">disks) during the prime generation; this gives the random number</span><br><span class="line">generator a better chance to gain enough entropy.</span><br><span class="line">We need to generate a lot of random bytes. It is a good idea to perform</span><br><span class="line">some other action (<span class="built_in">type</span> on the keyboard, move the mouse, utilize the</span><br><span class="line">disks) during the prime generation; this gives the random number</span><br><span class="line">generator a better chance to gain enough entropy.</span><br><span class="line">gpg: key B56B3F7B2A11A0D6 marked as ultimately trusted</span><br><span class="line">gpg: directory <span class="string">&#x27;/root/.gnupg/openpgp-revocs.d&#x27;</span> created</span><br><span class="line">gpg: revocation certificate stored as <span class="string">&#x27;/root/.gnupg/openpgp-revocs.d/52A05B5F40C7F10EA56D3A38B56B3F7B2A11A0D6.rev&#x27;</span></span><br><span class="line">public and secret key created and signed.</span><br><span class="line"></span><br><span class="line">pub   rsa3072 2022-02-07 [SC] [expires: 2024-02-07]</span><br><span class="line">      52A05B5F40C7F10EA56D3A38B56B3F7B2A11A0D6</span><br><span class="line">uid                      user-name &lt;user-name@example.com&gt;</span><br><span class="line">sub   rsa3072 2022-02-07 [E] [expires: 2024-02-07]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check your key</span></span><br><span class="line">$ gpg --fingerprint</span><br><span class="line">gpg: checking the trustdb</span><br><span class="line">gpg: marginals needed: 3  completes needed: 1  trust model: pgp</span><br><span class="line">gpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 1u</span><br><span class="line">gpg: next trustdb check due at 2024-02-07</span><br><span class="line">/root/.gnupg/pubring.kbx</span><br><span class="line">------------------------</span><br><span class="line">pub   rsa3072 2022-02-07 [SC] [expires: 2024-02-07]</span><br><span class="line">      52A0 5B5F 40C7 F10E A56D  3A38 B56B 3F7B 2A11 A0D6</span><br><span class="line">uid           [ultimate] user-name &lt;user-name@example.com&gt;</span><br><span class="line">sub   rsa3072 2022-02-07 [E] [expires: 2024-02-07]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Export your private key (for CICD)</span></span><br><span class="line"><span class="comment"># Method 1</span></span><br><span class="line">$ gpg --export-secret-key --armor <span class="string">&quot;user-name&quot;</span> &gt; private.key</span><br><span class="line"><span class="comment"># Method 2</span></span><br><span class="line">$ gpg --export-secret-key --armor <span class="string">&quot;<span class="variable">$&#123;KEY_FP&#125;</span>&quot;</span> &gt; private.key</span><br><span class="line"></span><br><span class="line"><span class="comment"># Export your public key (for CICD)</span></span><br><span class="line">$ gpg --<span class="built_in">export</span> --armor <span class="string">&quot;<span class="variable">$&#123;KEY_FP&#125;</span>&quot;</span> &gt; public.key</span><br></pre></td></tr></table></figure>

<h3 id="To-use-keys-for-encryption"><a href="#To-use-keys-for-encryption" class="headerlink" title="To use keys for encryption"></a>To use keys for encryption</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import keys (in CICD, otherwise skip it)</span></span><br><span class="line">$ gpg --import public.key</span><br><span class="line">$ gpg --import private.key</span><br></pre></td></tr></table></figure>

<p>Set fingerprint in <code>.sops.yaml</code>, which is under the same folder of your secrets.yaml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">creation_rules:</span></span><br><span class="line">    <span class="comment"># encrypted using user-name &lt;user-name@example.com&gt;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">pgp:</span> <span class="string">&quot;52A05B5F40C7F10EA56D3A38B56B3F7B2A11A0D6&quot;</span></span><br></pre></td></tr></table></figure>
<p>or <code>export SOPS_PGP_FP=&quot;52A05B5F40C7F10EA56D3A38B56B3F7B2A11A0D6&quot;</code></p>
<p>Encryption and Decryption</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Encryption and Decryption of helm secrets is a wrapper of sops</span></span><br><span class="line"><span class="comment"># Encryption (secrets.yaml will be encrypted)</span></span><br><span class="line">$ helm secrets enc secrets.yaml</span><br><span class="line">$ cat secrets.yaml</span><br><span class="line">podAnnotations:</span><br><span class="line">    secret: ENC[AES256_GCM,data:R5pFRNs=,iv:2OF6j34rusZYulfkalXlgOs4+3M9t57R3rJwy/NJKos=,tag:SQTMAQg1HsTjdnAqeEWWnQ==,<span class="built_in">type</span>:str]</span><br><span class="line">sops:</span><br><span class="line">    kms: []</span><br><span class="line">    gcp_kms: []</span><br><span class="line">    azure_kv: []</span><br><span class="line">    hc_vault: []</span><br><span class="line">    age: []</span><br><span class="line">    lastmodified: <span class="string">&quot;2021-10-04T16:34:43Z&quot;</span></span><br><span class="line">    mac: ENC[AES256_GCM,data:BZLaP0aV0xqU6863VULSFnFiZkDvaWNT91mBvUxbgQVjUQkPwNCtj7bFx7zRLutf684xr6Xvx7EjRc0KmA/q7w9elLpj5XP6lvHwVcw2dYwnc/nXMvfAcHQTv0Gl3Ey+PXZ7lECouciZtyKd9ib9IEMnKrwzqs4ZDPC9Y6DZitU=,iv:BQk4/siDQxkemO3g3SEGzoeuka5BmoAL/23oRT4sM60=,tag:Ef9tL6h2MNAAV7o2RI3SPg==,<span class="built_in">type</span>:str]</span><br><span class="line">    pgp:</span><br><span class="line">        - created_at: <span class="string">&quot;2021-10-04T16:34:43Z&quot;</span></span><br><span class="line">          enc: |</span><br><span class="line">            -----BEGIN PGP MESSAGE-----</span><br><span class="line">            hQEMA9ce5qCwOO4MAQf/UI8ggX3hR0ZrVeZ4j5MiYsl7O1lDAS6xWLGivRfOfy4l</span><br><span class="line">            UYBMZi9E7LYNN47xXgbbGUJ8MXrCEp+vQR0AUqG+K/X6OPP6pmeeAlEGH0o9Fab0</span><br><span class="line">            0f0sU3/h9juST0RBtTDa8YTmjTglD5uAzjYNqVsYe0YLNv6HxDw6Fu/h/sXI3Ekn</span><br><span class="line">            PCYw3E+ONjOAQWfCGgkiIQkdPmnB0kZD+bA3U+3EGSnPPljTWYyGuGyonEm4IckV</span><br><span class="line">            AGgzhtsPWKmh/SwVa603eVD/+JvBzszyUao9JinijZHJJmcHJg6TjuOOUUlTmRbq</span><br><span class="line">            8Fgf3NUE3G5BQgeH1nFzLzlNYg6MVSceaUIX7vilwtJeAVHt2EIxGz6oZO0vFGYc</span><br><span class="line">            NoACwB6FkVEp3jS4QR0wMhtaflpGtoaooc+BIWxrbf9S0XIv0RHdf33/X7vbMRsz</span><br><span class="line">            tUw10Hsbl13DeySp+6uwoom3VVGCuisQdewoIf1ntg==</span><br><span class="line">            =c0YW</span><br><span class="line">            -----END PGP MESSAGE-----</span><br><span class="line">          fp: D6174A02027050E59C711075B430C4E58E2BBBA3</span><br><span class="line">    unencrypted_suffix: _unencrypted</span><br><span class="line">    version: 3.7.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decryption</span></span><br><span class="line">$ helm secrets dec secrets.yaml</span><br><span class="line">secrets.yaml.dec will be generated</span><br><span class="line">$ cat secrets.yaml.dec</span><br><span class="line">podAnnotations:</span><br><span class="line">    secret: value</span><br><span class="line"></span><br><span class="line"><span class="comment"># View decryption in console</span></span><br><span class="line">$ helm secrets view secrets.yaml</span><br><span class="line">podAnnotations:</span><br><span class="line">    secret: value</span><br><span class="line"></span><br><span class="line"><span class="comment"># Edit your secret value</span></span><br><span class="line">$ helm secrets edit secrets.yaml</span><br></pre></td></tr></table></figure>

<h3 id="To-install-helm-chart-with-secrets"><a href="#To-install-helm-chart-with-secrets" class="headerlink" title="To install helm chart with secrets"></a>To install helm chart with secrets</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ helm secrets upgrade \</span><br><span class="line">  helloworld \</span><br><span class="line">  stable/java-app \</span><br><span class="line">  --install \</span><br><span class="line">  --timeout 600 \</span><br><span class="line">  --<span class="built_in">wait</span> \</span><br><span class="line">  --kube-context=sandbox \</span><br><span class="line">  --namespace=projectx \</span><br><span class="line">  --<span class="built_in">set</span> global.app_version=bff8fc4 \</span><br><span class="line">  -f helm_vars/projectx/sandbox/us-east-1/java-app/helloworld/secrets.yaml \</span><br><span class="line">  -f helm_vars/projectx/sandbox/us-east-1/java-app/helloworld/values.yaml \</span><br><span class="line">  -f helm_vars/secrets.yaml \</span><br><span class="line">  -f helm_vars/values.yaml</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://helm.sh/">https://helm.sh</a></li>
<li><a href="https://github.com/chartmuseum/helm-push">https://github.com/chartmuseum/helm-push</a></li>
<li><a href="https://github.com/jkroepke/helm-secrets/wiki/Usage">https://github.com/jkroepke/helm-secrets/wiki/Usage</a></li>
</ol>
]]></content>
      <categories>
        <category>helm</category>
      </categories>
      <tags>
        <tag>helm</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>Install ceph with cephadm</title>
    <url>/2021/10/05/cephadm/</url>
    <content><![CDATA[<h2 id="Layout"><a href="#Layout" class="headerlink" title="Layout"></a>Layout</h2><p>10.0.2.7 iam02 /dev/sda mon<br>10.0.2.4 openam01 /dev/sdc mon<br>10.0.2.5 opendj01 /dev/sdb mon</p>
<span id="more"></span>

<h2 id="Prepare-Environment"><a href="#Prepare-Environment" class="headerlink" title="Prepare Environment"></a>Prepare Environment</h2><ul>
<li>ntp</li>
<li>python3 </li>
<li>container runtime</li>
<li>ssh environment</li>
<li>repo source  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ wget --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm</span><br><span class="line">$ chmod +x cephadm</span><br><span class="line">$ ./cephadm add-repo --release octopus</span><br><span class="line">$ ./cephadm install</span><br><span class="line">$ apt-get update</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>Note: Try to install with root user to avoid some issues</p>
<h3 id="1-Initialization"><a href="#1-Initialization" class="headerlink" title="1. Initialization"></a>1. Initialization</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cephadm bootstrap --mon-ip 10.0.2.7  --cluster-network 10.0.2.0/24</span><br><span class="line"><span class="comment"># Note: store the dashboard login credentials in the initialization logs</span></span><br><span class="line"><span class="comment"># create user for dashboard</span></span><br><span class="line">$ ceph dashboard set-login-credentials yuanjing -i secret-yuanjing</span><br></pre></td></tr></table></figure>
<h3 id="2-Access-cluster"><a href="#2-Access-cluster" class="headerlink" title="2. Access cluster"></a>2. Access cluster</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check cluster status</span></span><br><span class="line">$ cephadm shell -- ceph -s</span><br><span class="line"><span class="comment"># check osd status</span></span><br><span class="line">$ cephadm shell -- ceph osd tree</span><br></pre></td></tr></table></figure>

<h3 id="3-Add-other-nodes-to-cluster"><a href="#3-Add-other-nodes-to-cluster" class="headerlink" title="3. Add other nodes to cluster"></a>3. Add other nodes to cluster</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># copy ssh pub key (ceph.pub) generated by ceph to other nodes</span></span><br><span class="line">$ ssh-copy-id -f -i /etc/ceph/ceph.pub openam01</span><br><span class="line">$ ssh-copy-id -f -i /etc/ceph/ceph.pub opendj01</span><br><span class="line"><span class="comment"># add nodes</span></span><br><span class="line">$ ceph orch host add openam01 10.0.2.4</span><br><span class="line">$ ceph orch host add opendj01 10.0.2.5</span><br><span class="line"></span><br><span class="line"><span class="comment"># check cluster status (mon and mgr will be deployed on nodes automatically)</span></span><br><span class="line">$ ceph -s</span><br></pre></td></tr></table></figure>
<h3 id="4-Deploy-osd"><a href="#4-Deploy-osd" class="headerlink" title="4. Deploy osd"></a>4. Deploy osd</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check available devices</span></span><br><span class="line">$ os orch device ls</span><br><span class="line"><span class="comment"># A storage device is considered available if all of the following conditions are met:</span></span><br><span class="line"><span class="comment"># The device must have no partitions.</span></span><br><span class="line"><span class="comment"># The device must not have any LVM state.</span></span><br><span class="line"><span class="comment"># The device must not be mounted.</span></span><br><span class="line"><span class="comment"># The device must not contain a file system.</span></span><br><span class="line"><span class="comment"># The device must not contain a Ceph BlueStore OSD.</span></span><br><span class="line"><span class="comment"># The device must be larger than 5 GB.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># deploy all available devices</span></span><br><span class="line">$ ceph orch apply osd --all-available-devices</span><br><span class="line"><span class="comment"># or add one by one</span></span><br><span class="line">$ ceph orch daemon add osd iam02:/dev/sda</span><br><span class="line">$ ceph orch daemon add osd openam01:/dev/sdc </span><br><span class="line">$ ceph orch daemon add osd opendj01:/dev/sdcb</span><br><span class="line"></span><br><span class="line"><span class="comment"># add new osd</span></span><br><span class="line">$ ceph orch daemon add osd hostname:/dev/sdb</span><br></pre></td></tr></table></figure>
<h3 id="5-Deploy-mds-CephFS"><a href="#5-Deploy-mds-CephFS" class="headerlink" title="5. Deploy mds (CephFS)"></a>5. Deploy mds (CephFS)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create pool for data and metadata </span></span><br><span class="line">$ ceph osd pool create cephfs_data 64 64</span><br><span class="line">$ ceph osd pool create cephfs_metadata 64 64</span><br><span class="line"><span class="comment"># create a fs</span></span><br><span class="line">$ ceph fs new cephfs-demo1 cephfs_metadata cephfs_data</span><br><span class="line"><span class="comment"># fs service</span></span><br><span class="line">$ ceph orch apply mds cephfs-demo1 --placement=<span class="string">&quot;3 iam02 openam01 opendj01&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># or create with one command</span></span><br><span class="line"><span class="comment"># pool will be automatically created in this case with default pg and pgp</span></span><br><span class="line">$ ceph fs volume create cephfs-demo2 <span class="string">&quot;iam02,openam01,opendj01&quot;</span></span><br><span class="line"><span class="comment"># check</span></span><br><span class="line">$ ceph fs ls</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="6-Deploy-rgw-Ceph-Object-Storage"><a href="#6-Deploy-rgw-Ceph-Object-Storage" class="headerlink" title="6. Deploy rgw (Ceph Object Storage)"></a>6. Deploy rgw (Ceph Object Storage)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create realm, zonegroup, zone</span></span><br><span class="line"><span class="comment"># radosgw-admin realm create --rgw-realm=&lt;realm-name&gt; --default</span></span><br><span class="line">$ radosgw-admin realm create --rgw-realm=rgw01 --default</span><br><span class="line"><span class="comment"># radosgw-admin zonegroup create --rgw-zonegroup=&lt;zonegroup-name&gt;  --master --default</span></span><br><span class="line">$ radosgw-admin zonegroup create --rgw-zonegroup=<span class="built_in">test</span> --master --default</span><br><span class="line"><span class="comment"># radosgw-admin zone create --rgw-zonegroup=&lt;zonegroup-name&gt; --rgw-zone=&lt;zone-name&gt; --master --default</span></span><br><span class="line">$ radosgw-admin zone create --rgw-zonegroup=<span class="built_in">test</span> --rgw-zone=room1 --master --default</span><br><span class="line"><span class="comment"># above commands will generate .rgw.root</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ceph orch apply rgw *&lt;realm-name&gt;* *&lt;zone-name&gt;* --placement=&quot;*&lt;num-daemons&gt;* [*&lt;host1&gt;* ...]&quot;</span></span><br><span class="line">$ ceph orch apply rgw myrgw --realm_name=rgw01 --zone_name= room1 --placement=<span class="string">&quot;2 iam02 openam01&quot;</span></span><br><span class="line"><span class="comment"># above command will generate </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># modify crush rule of rgw pool</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> .rgw.root crush_rule on-ssd</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a user</span></span><br><span class="line">$ radosgw-admin user create --uid=myname --display-name=My Name --system</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="7-Deploy-block-stoage"><a href="#7-Deploy-block-stoage" class="headerlink" title="7. Deploy block stoage"></a>7. Deploy block stoage</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create pool</span></span><br><span class="line">$ ceph osd pool create blockpool 64 64 on-ssd</span><br><span class="line">❯ ceph osd pool create bench.hdd 128 128 on-hdd</span><br><span class="line"><span class="comment"># enable rdb application</span></span><br><span class="line">$ ceph osd pool application <span class="built_in">enable</span> blockpool rbd</span><br><span class="line"></span><br><span class="line"><span class="comment"># create rbd image</span></span><br><span class="line">$ rbd create -p blockpool --image rbd-demo.img --size 10G</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">$ rbd create blockpool/rbd-demo.img  --size 10G</span><br><span class="line"></span><br><span class="line"><span class="comment"># check </span></span><br><span class="line">$ rbd info blockpool/rbd-demo.img</span><br><span class="line"><span class="comment"># map</span></span><br><span class="line">rbd map blockpool/rbd-demo.img</span><br><span class="line"><span class="comment"># if failed, kernel only supports layering, disable others</span></span><br><span class="line">$ rbd feature <span class="built_in">disable</span> blockpool/rbd-demo.img  deep-flatten</span><br><span class="line"><span class="comment">#  backforth</span></span><br><span class="line">$ ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># check disk info</span></span><br><span class="line">$ fdisk -l</span><br><span class="line"><span class="comment"># mount to  a local dir</span></span><br><span class="line">$ mkdir /mnt/rbd-demo</span><br><span class="line">$ mkfs.xfs /dev/rbd0</span><br><span class="line">$ mount /dev/rbd0 /mnt/rbd-demo</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Ceph-CLI"><a href="#Ceph-CLI" class="headerlink" title="Ceph CLI"></a>Ceph CLI</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># install ceph-common on client nodes</span></span><br><span class="line">$ cephadm add repo --release &#123;pacific&#125;</span><br><span class="line">$ cephadm install ceph-common</span><br><span class="line"><span class="comment"># on client nodes, we could use ceph command</span></span><br><span class="line">$ ceph -s</span><br><span class="line">$ ceph orch host ls</span><br></pre></td></tr></table></figure>




<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://www.cnblogs.com/zjz20/p/14136349.html">https://www.cnblogs.com/zjz20/p/14136349.html</a></li>
</ol>
]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>cloud</tag>
        <tag>ceph</tag>
        <tag>cephadm</tag>
      </tags>
  </entry>
  <entry>
    <title>rook-ceph installation</title>
    <url>/2021/10/06/rook-ceph/</url>
    <content><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This article is about</p>
<ul>
<li>What are the preparations for rook-ceph installation</li>
<li>How to install rook-operator</li>
<li>How to create a ceph cluster</li>
<li>How to install rook-ceph with helm charts</li>
</ul>
<span id="more"></span>

<h2 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h2><p>For installing ceph in k8s with rook, follwing requirements should be fulfilled</p>
<ul>
<li>kubernetes &gt; v1.17.0</li>
<li>minimum 3 nodes of kubernetes cluster</li>
<li>disk  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1. The device must have no partitions.</span><br><span class="line">2. The device must not have any LVM state.</span><br><span class="line">3. The device must not be mounted.</span><br><span class="line">4. The device must not contain a file system.</span><br><span class="line">5. The device must not contain a Ceph BlueStore OSD.</span><br><span class="line">6. The device must be larger than 5 GB.</span><br><span class="line"><span class="comment"># check with lsblk</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Install-Rook-Operator"><a href="#Install-Rook-Operator" class="headerlink" title="Install Rook Operator"></a>Install Rook Operator</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># download rook release 1.3</span></span><br><span class="line">$ git <span class="built_in">clone</span> --single-branch --branch release-1.3 https://github.com/rook/rook.git</span><br><span class="line"><span class="comment"># configuration files</span></span><br><span class="line">$ <span class="built_in">cd</span> rook/cluster/examples/kubernetes/ceph</span><br><span class="line"></span><br><span class="line">$ kubectl create -f common.yaml</span><br><span class="line">$ kubectl apply -f operator.yaml</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">$ configmap/rook-ceph-operator-config created</span><br><span class="line">$ deployment.apps/rook-ceph-operator created</span><br><span class="line"></span><br><span class="line"><span class="comment">#check pods created in rook-ceph</span></span><br><span class="line">$ kubectl get pod -n rook-ceph</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">rook-ceph-operator-599765ff49-fhbz9   1/1     Running   0          92s</span><br><span class="line">rook-discover-6fhlb                   1/1     Running   0          55s</span><br><span class="line">rook-discover-97kmz                   1/1     Running   0          55s</span><br><span class="line">rook-discover-z5k2z                   1/1     Running   0          55s</span><br></pre></td></tr></table></figure>

<h2 id="Create-a-ceph-cluster"><a href="#Create-a-ceph-cluster" class="headerlink" title="Create a ceph cluster"></a>Create a ceph cluster</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># label your node for ceph setup</span></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl taint node k8smaster01 node-role.kubernetes.io/master=&quot;&quot;:NoSchedule</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl taint node k8smaster02 node-role.kubernetes.io/master=&quot;&quot;:NoSchedule</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl taint node k8smaster03 node-role.kubernetes.io/master=&quot;&quot;:NoSchedule</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl label nodes &#123;192.168.10.127,192.168.10.128,192.168.10.129&#125; ceph-osd=enabled</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl label nodes &#123;192.168.10.127,192.168.10.128,192.168.10.129&#125; ceph-mon=enabled</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl label nodes 192.168.10.127 ceph-mgr=enabled</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># apply resources</span></span><br><span class="line">[root@k8smaster01 ~]<span class="comment"># cd /root/rook/cluster/examples/kubernetes/ceph/</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl create -f common.yaml</span></span><br><span class="line"></span><br><span class="line">[root@k8smaster01 ceph]<span class="comment"># kubectl create -f operator.yaml</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">kubectl apply -f cephcluster.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">NAME                                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">csi-cephfsplugin-lz6dn                                 3/3     Running   0          3m54s</span><br><span class="line">csi-cephfsplugin-provisioner-674847b584-4j9jw          5/5     Running   0          3m54s</span><br><span class="line">csi-cephfsplugin-provisioner-674847b584-h2cgl          5/5     Running   0          3m54s</span><br><span class="line">csi-cephfsplugin-qbpnq                                 3/3     Running   0          3m54s</span><br><span class="line">csi-cephfsplugin-qzsvr                                 3/3     Running   0          3m54s</span><br><span class="line">csi-rbdplugin-kk9sw                                    3/3     Running   0          3m55s</span><br><span class="line">csi-rbdplugin-l95f8                                    3/3     Running   0          3m55s</span><br><span class="line">csi-rbdplugin-provisioner-64ccb796cf-8gjwv             6/6     Running   0          3m55s</span><br><span class="line">csi-rbdplugin-provisioner-64ccb796cf-dhpwt             6/6     Running   0          3m55s</span><br><span class="line">csi-rbdplugin-v4hk6                                    3/3     Running   0          3m55s</span><br><span class="line">rook-ceph-crashcollector-pool-33zy7-68cdfb6bcf-9cfkn   1/1     Running   0          109s</span><br><span class="line">rook-ceph-crashcollector-pool-33zyc-565559f7-7r6rt     1/1     Running   0          53s</span><br><span class="line">rook-ceph-crashcollector-pool-33zym-749dcdc9df-w4xzl   1/1     Running   0          78s</span><br><span class="line">rook-ceph-mgr-a-7fdf77cf8d-ppkwl                       1/1     Running   0          53s</span><br><span class="line">rook-ceph-mon-a-97d9767c6-5ftfm                        1/1     Running   0          109s</span><br><span class="line">rook-ceph-mon-b-9cb7bdb54-lhfkj                        1/1     Running   0          96s</span><br><span class="line">rook-ceph-mon-c-786b9f7f4b-jdls4                       1/1     Running   0          78s</span><br><span class="line">rook-ceph-operator-599765ff49-fhbz9                    1/1     Running   0          6m58s</span><br><span class="line">rook-ceph-osd-prepare-pool-33zy7-c2hww                 1/1     Running   0          21s</span><br><span class="line">rook-ceph-osd-prepare-pool-33zyc-szwsc                 1/1     Running   0          21s</span><br><span class="line">rook-ceph-osd-prepare-pool-33zym-2p68b                 1/1     Running   0          21s</span><br><span class="line">rook-discover-6fhlb                                    1/1     Running   0          6m21s</span><br><span class="line">rook-discover-97kmz                                    1/1     Running   0          6m21s</span><br><span class="line">rook-discover-z5k2z                                    1/1     Running   0          6m21s</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Create-a-block-storageclass"><a href="#Create-a-block-storageclass" class="headerlink" title="Create a block storageclass"></a>Create a block storageclass</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f ./csi/rbd/storageclass.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">cephblockpool.ceph.rook.io/replicapool created</span><br><span class="line">storageclass.storage.k8s.io/rook-ceph-block created</span><br></pre></td></tr></table></figure>


<h2 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h2><ol>
<li><p><a href="https://kuboard.cn/learning/k8s-intermediate/persistent/ceph/rook-config.html#%E5%AE%89%E8%A3%85-rook-ceph">https://kuboard.cn/learning/k8s-intermediate/persistent/ceph/rook-config.html#%E5%AE%89%E8%A3%85-rook-ceph</a></p>
</li>
<li><p><a href="https://www.joyk.com/dig/detail/1585287051590150">https://www.joyk.com/dig/detail/1585287051590150</a></p>
</li>
<li><p><a href="https://www.qikqiak.com/post/deploy-ceph-cluster-with-rook/">https://www.qikqiak.com/post/deploy-ceph-cluster-with-rook/</a></p>
</li>
<li><p><a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-ceph-cluster-within-kubernetes-using-rook">https://www.digitalocean.com/community/tutorials/how-to-set-up-a-ceph-cluster-within-kubernetes-using-rook</a></p>
</li>
<li><p><a href="https://xiebiao.top/post/storage/rook_install_operator/">https://xiebiao.top/post/storage/rook_install_operator/</a></p>
</li>
<li><p><a href="https://github.com/rook/rook/tree/master/cluster/examples">https://github.com/rook/rook/tree/master/cluster/examples</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>ceph</category>
      </categories>
      <tags>
        <tag>cloud</tag>
        <tag>ceph</tag>
        <tag>rook-ceph</tag>
      </tags>
  </entry>
  <entry>
    <title>gitlab</title>
    <url>/2022/02/06/gitlab/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>helmfile</title>
    <url>/2022/02/06/helmfile/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>helm</category>
      </categories>
      <tags>
        <tag>helm</tag>
        <tag>k8s</tag>
        <tag>helm charts</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s cluster installation with kubeadm</title>
    <url>/2022/02/06/k8s/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>container</tag>
      </tags>
  </entry>
  <entry>
    <title>kubeflow</title>
    <url>/2022/02/06/kubeflow/</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
